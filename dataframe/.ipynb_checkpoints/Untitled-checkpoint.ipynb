{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8710e930-b33e-4ff1-aa29-028a7d695c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Window, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName('sample').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5ab943d0-da92-4770-b77e-8ba1e3dd11f8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-------------+------+\n",
      "|  id|department|employee_name|salary|\n",
      "+----+----------+-------------+------+\n",
      "|   1|         1|         John|  5000|\n",
      "|   2|         1|        Alice|  6000|\n",
      "|   3|         1|          Bob|  7000|\n",
      "|   4|         2|      Charlie|  5500|\n",
      "|   5|         2|        David|  6500|\n",
      "|   6|         3|          Eva|  7500|\n",
      "|   7|         3|        Frank|  8000|\n",
      "|   8|         3|        Grace|  9000|\n",
      "|   1|         1|         John|  5000|\n",
      "|NULL|         1|          Sam|  8000|\n",
      "+----+----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 1, 'John', 5000),\n",
    "        (2, 1, 'Alice', 6000),\n",
    "        (3, 1, 'Bob', 7000),\n",
    "        (4, 2, 'Charlie', 5500),\n",
    "        (5, 2, 'David', 6500),\n",
    "        (6, 3, 'Eva', 7500),\n",
    "        (7, 3, 'Frank', 8000),\n",
    "        (8, 3, 'Grace', 9000),\n",
    "       (1, 1, 'John', 5000),\n",
    "       (None, 1, 'Sam', 8000)]\n",
    "\n",
    "columns = ['id', 'department', 'employee_name', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "decee50d-d15f-4068-94a2-003f03af5380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------+------+\n",
      "| id|department|employee_name|salary|\n",
      "+---+----------+-------------+------+\n",
      "|  1|         1|         John|  5000|\n",
      "|  2|         1|        Alice|  6000|\n",
      "|  3|         1|          Bob|  7000|\n",
      "|  4|         2|      Charlie|  5500|\n",
      "|  5|         2|        David|  6500|\n",
      "|  6|         3|          Eva|  7500|\n",
      "|  7|         3|        Frank|  8000|\n",
      "|  8|         3|        Grace|  9000|\n",
      "+---+----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.filter(col('id').isNull()).drop()\n",
    "df2 = df.subtract(df1)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bed8dce5-98f0-494e-ae99-44bedfe78efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, department: bigint, employee_name: string, salary: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b0546c7b-e506-414c-a176-2049163405a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------+------+\n",
      "| id|department|employee_name|salary|\n",
      "+---+----------+-------------+------+\n",
      "+---+----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.subtract(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6616d5bb-3cd3-454c-b0a4-e37b6b639685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-------------+------+\n",
      "|  id|department|employee_name|salary|\n",
      "+----+----------+-------------+------+\n",
      "|NULL|         1|          Sam|  8000|\n",
      "+----+----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c2ff0ee-4355-48ee-9489-8de487aecc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------+------+\n",
      "| id|department|employee_name|salary|\n",
      "+---+----------+-------------+------+\n",
      "|  1|         1|         John|  5000|\n",
      "|  2|         1|        Alice|  6000|\n",
      "|  3|         1|          Bob|  7000|\n",
      "|  4|         2|      Charlie|  5500|\n",
      "|  5|         2|        David|  6500|\n",
      "|  6|         3|          Eva|  7500|\n",
      "|  7|         3|        Frank|  8000|\n",
      "|  8|         3|        Grace|  9000|\n",
      "+---+----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df = df.na.drop(subset='id').drop_duplicates()\n",
    "df = df.na.drop(subset='id').dropDuplicates()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a42e04c-a660-406b-9240-61b2e197ceac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Name: Basudev\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "e\n",
      "u\n"
     ]
    }
   ],
   "source": [
    "name = input(\"Name:\")\n",
    "li = ['a', 'e', 'i', 'o', 'u']\n",
    "for i in li:\n",
    "\tif i in name:\n",
    "\t\tprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1230819a-c24e-44af-9bd4-c77e7baf648f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'e', 'u']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li1 = [i for i in li if i in name]\n",
    "li1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa7ed7b8-b435-4ab5-b073-3bf37c80509b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------+------+-----------+\n",
      "| id|department|employee_name|salary|rank_number|\n",
      "+---+----------+-------------+------+-----------+\n",
      "|  1|         1|         John|  5000|          3|\n",
      "|  6|         3|          Eva|  7500|          3|\n",
      "+---+----------+-------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy('department').orderBy(col('salary').desc())\n",
    "df1 = df.withColumn('rank_number', row_number().over(window))\n",
    "df1 = df1.filter(col('rank_number') == 3)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "498e5e7c-5598-409f-a461-72d9a410d863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------+-----------+\n",
      "| id|department|employee_name|rank_number|\n",
      "+---+----------+-------------+-----------+\n",
      "|  9|         2|       Parkin|       8000|\n",
      "+---+----------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_row = Row(9, 2, 'Parkin', 8000)\n",
    "columns = ('id', 'department', 'employee_name', 'rank_number')\n",
    "df2 = spark.createDataFrame([new_row], schema = columns)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dae84b56-cb5d-45c3-a6a5-6ad4330b2827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------+------+\n",
      "| id|department|employee_name|salary|\n",
      "+---+----------+-------------+------+\n",
      "|  1|         1|         John|  5000|\n",
      "|  2|         1|        Alice|  6000|\n",
      "|  3|         1|          Bob|  7000|\n",
      "|  4|         2|      Charlie|  5500|\n",
      "|  5|         2|        David|  6500|\n",
      "|  6|         3|          Eva|  7500|\n",
      "|  7|         3|        Frank|  8000|\n",
      "|  8|         3|        Grace|  9000|\n",
      "|  9|         2|       Parkin|  8000|\n",
      "+---+----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.union(df2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1514e6a3-36fb-46af-985a-fb097146da9f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/basu/spark/python/pyspark/broadcast.py\", line 183, in dump\n",
      "    pickle.dump(value, f, pickle_protocol)\n",
      "  File \"/home/basu/spark/python/pyspark/rdd.py\", line 386, in __getnewargs__\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [RDD_TRANSFORM_ONLY_VALID_ON_DRIVER] It appears that you are attempting to broadcast an RDD or reference an RDD from an \n",
      "action or transformation. RDD transformations and actions can only be invoked by the \n",
      "driver, not inside of other transformations; for example, \n",
      "rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values \n",
      "transformation and count action cannot be performed inside of the rdd1.map \n",
      "transformation. For more information, see SPARK-5063.\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize broadcast: PySparkRuntimeError: [RDD_TRANSFORM_ONLY_VALID_ON_DRIVER] It appears that you are attempting to broadcast an RDD or reference an RDD from an \naction or transformation. RDD transformations and actions can only be invoked by the \ndriver, not inside of other transformations; for example, \nrdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values \ntransformation and count action cannot be performed inside of the rdd1.map \ntransformation. For more information, see SPARK-5063.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/spark/python/pyspark/broadcast.py:183\u001b[0m, in \u001b[0;36mBroadcast.dump\u001b[0;34m(self, value, f)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPickleError:\n",
      "File \u001b[0;32m~/spark/python/pyspark/rdd.py:386\u001b[0m, in \u001b[0;36mRDD.__getnewargs__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getnewargs__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;66;03m# This method is called when attempting to pickle an RDD, which is always an error:\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    387\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD_TRANSFORM_ONLY_VALID_ON_DRIVER\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    388\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    389\u001b[0m     )\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [RDD_TRANSFORM_ONLY_VALID_ON_DRIVER] It appears that you are attempting to broadcast an RDD or reference an RDD from an \naction or transformation. RDD transformations and actions can only be invoked by the \ndriver, not inside of other transformations; for example, \nrdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values \ntransformation and count action cannot be performed inside of the rdd1.map \ntransformation. For more information, see SPARK-5063.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]\n\u001b[1;32m      2\u001b[0m rdd \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mparallelize(data)\n\u001b[0;32m----> 3\u001b[0m broadcast_variable \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark/python/pyspark/context.py:1765\u001b[0m, in \u001b[0;36mSparkContext.broadcast\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbroadcast\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: T) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBroadcast[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1736\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1737\u001b[0m \u001b[38;5;124;03m    Broadcast a read-only variable to the cluster, returning a :class:`Broadcast`\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;124;03m    object for reading it in distributed functions. The variable will\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1763\u001b[0m \u001b[38;5;124;03m    >>> bc.destroy()\u001b[39;00m\n\u001b[1;32m   1764\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1765\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBroadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pickled_broadcast_vars\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark/python/pyspark/broadcast.py:135\u001b[0m, in \u001b[0;36mBroadcast.__init__\u001b[0;34m(self, sc, value, pickle_registry, path, sock_file)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# no encryption, we can just write pickled data directly to the file from python\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     broadcast_out \u001b[38;5;241m=\u001b[39m f\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbroadcast_out\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_encryption_enabled:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_broadcast\u001b[38;5;241m.\u001b[39mwaitTillDataReceived()\n",
      "File \u001b[0;32m~/spark/python/pyspark/broadcast.py:189\u001b[0m, in \u001b[0;36mBroadcast.dump\u001b[0;34m(self, value, f)\u001b[0m\n\u001b[1;32m    187\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize broadcast: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[1;32m    188\u001b[0m     print_exec(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPicklingError(msg)\n\u001b[1;32m    190\u001b[0m f\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize broadcast: PySparkRuntimeError: [RDD_TRANSFORM_ONLY_VALID_ON_DRIVER] It appears that you are attempting to broadcast an RDD or reference an RDD from an \naction or transformation. RDD transformations and actions can only be invoked by the \ndriver, not inside of other transformations; for example, \nrdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values \ntransformation and count action cannot be performed inside of the rdd1.map \ntransformation. For more information, see SPARK-5063."
     ]
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "broadcast_variable = sc.broadcast(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20dc7f3a-1ff9-44be-86d7-7c06a8f4d986",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Broadcast the RDD\n",
    "broadcast_variable = sc.broadcast(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e759984-8769-45fe-b7ca-6668aa4b0131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcast_variable.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ff7e3d9-597c-48f3-a62c-3abad842c6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|    Basu|\n",
      "|  2|Prashant|\n",
      "|  3|Agnibesh|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    Row(1, 'Basu'),\n",
    "    Row(2, 'Prashant'),\n",
    "    Row(3, 'Agnibesh')\n",
    "]\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType(), True),\n",
    "    StructField('name', StringType(), True)])\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5ff61dc0-9e2f-4bfc-9ab1-c4fdaae15c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  1| Dev|\n",
      "|  5|Bala|\n",
      "|  3|Agni|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    Row(1, 'Dev'),\n",
    "    Row(5, 'Bala'),\n",
    "    Row(3, 'Agni')\n",
    "]\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType(), True),\n",
    "    StructField('name', StringType(), True)])\n",
    "df1 = spark.createDataFrame(data=data, schema=schema)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd8801a8-86cc-4cba-98e7-188d1064b63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_v = broadcast(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "53471793-4f7a-44ba-b65e-a79b6eaf4793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+\n",
      "| id|name|    name|\n",
      "+---+----+--------+\n",
      "|  1| Dev|    Basu|\n",
      "|  3|Agni|Agnibesh|\n",
      "+---+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(b_v, 'id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f7e5891d-5ab0-46c5-8e71-da761f5547cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  1| Dev|\n",
      "|  3|Agni|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(b_v, 'id', 'leftsemi').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e22e3041-0557-454e-b50a-7e81e21de291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+\n",
      "| id|name|    name|\n",
      "+---+----+--------+\n",
      "|  1| Dev|    Basu|\n",
      "|  5|Bala|    NULL|\n",
      "|  3|Agni|Agnibesh|\n",
      "+---+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(b_v, 'id', 'left').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1c990636-1142-454d-adff-b06d376f64eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/03 22:14:26 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build right for right outer join.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+\n",
      "| id|name|    name|\n",
      "+---+----+--------+\n",
      "|  1| Dev|    Basu|\n",
      "|  2|NULL|Prashant|\n",
      "|  3|Agni|Agnibesh|\n",
      "+---+----+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/03 22:14:27 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build right for right outer join.\n"
     ]
    }
   ],
   "source": [
    "df1.join(b_v, 'id', 'right').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a0a54dc0-a661-47ae-8288-ade8e576eaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/03 22:16:34 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build right for right outer join.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+\n",
      "| id|name|    name|\n",
      "+---+----+--------+\n",
      "|  1| Dev|    Basu|\n",
      "|  2|NULL|Prashant|\n",
      "|  3|Agni|Agnibesh|\n",
      "+---+----+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/03 22:16:34 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build right for right outer join.\n"
     ]
    }
   ],
   "source": [
    "df1.join(b_v, 'id', 'right_outer').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8e8b4f24-0f7c-494d-847f-8c5303a13c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/03 22:16:51 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build right for full outer join.\n",
      "24/02/03 22:16:52 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build right for full outer join.\n",
      "24/02/03 22:16:52 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build right for full outer join.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+\n",
      "| id|name|    name|\n",
      "+---+----+--------+\n",
      "|  1| Dev|    Basu|\n",
      "|  2|NULL|Prashant|\n",
      "|  3|Agni|Agnibesh|\n",
      "|  5|Bala|    NULL|\n",
      "+---+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(b_v, 'id', 'outer').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e2e7ddec-c2ff-4129-8f39-f2e067c1278a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method join in module pyspark.sql.dataframe:\n",
      "\n",
      "join(other: 'DataFrame', on: Union[str, List[str], pyspark.sql.column.Column, List[pyspark.sql.column.Column], NoneType] = None, how: Optional[str] = None) -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Joins with another :class:`DataFrame`, using the given join expression.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    other : :class:`DataFrame`\n",
      "        Right side of the join\n",
      "    on : str, list or :class:`Column`, optional\n",
      "        a string for the join column name, a list of column names,\n",
      "        a join expression (Column), or a list of Columns.\n",
      "        If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      "        the column(s) must exist on both sides, and this performs an equi-join.\n",
      "    how : str, optional\n",
      "        default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
      "        ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n",
      "        ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n",
      "        ``anti``, ``leftanti`` and ``left_anti``.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "        Joined DataFrame.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    The following performs a full outer join between ``df1`` and ``df2``.\n",
      "    \n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> from pyspark.sql.functions import desc\n",
      "    >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")]).toDF(\"age\", \"name\")\n",
      "    >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
      "    >>> df3 = spark.createDataFrame([Row(age=2, name=\"Alice\"), Row(age=5, name=\"Bob\")])\n",
      "    >>> df4 = spark.createDataFrame([\n",
      "    ...     Row(age=10, height=80, name=\"Alice\"),\n",
      "    ...     Row(age=5, height=None, name=\"Bob\"),\n",
      "    ...     Row(age=None, height=None, name=\"Tom\"),\n",
      "    ...     Row(age=None, height=None, name=None),\n",
      "    ... ])\n",
      "    \n",
      "    Inner join on columns (default)\n",
      "    \n",
      "    >>> df.join(df2, 'name').select(df.name, df2.height).show()\n",
      "    +----+------+\n",
      "    |name|height|\n",
      "    +----+------+\n",
      "    | Bob|    85|\n",
      "    +----+------+\n",
      "    >>> df.join(df4, ['name', 'age']).select(df.name, df.age).show()\n",
      "    +----+---+\n",
      "    |name|age|\n",
      "    +----+---+\n",
      "    | Bob|  5|\n",
      "    +----+---+\n",
      "    \n",
      "    Outer join for both DataFrames on the 'name' column.\n",
      "    \n",
      "    >>> df.join(df2, df.name == df2.name, 'outer').select(\n",
      "    ...     df.name, df2.height).sort(desc(\"name\")).show()\n",
      "    +-----+------+\n",
      "    | name|height|\n",
      "    +-----+------+\n",
      "    |  Bob|    85|\n",
      "    |Alice|  NULL|\n",
      "    | NULL|    80|\n",
      "    +-----+------+\n",
      "    >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).show()\n",
      "    +-----+------+\n",
      "    | name|height|\n",
      "    +-----+------+\n",
      "    |  Tom|    80|\n",
      "    |  Bob|    85|\n",
      "    |Alice|  NULL|\n",
      "    +-----+------+\n",
      "    \n",
      "    Outer join for both DataFrams with multiple columns.\n",
      "    \n",
      "    >>> df.join(\n",
      "    ...     df3,\n",
      "    ...     [df.name == df3.name, df.age == df3.age],\n",
      "    ...     'outer'\n",
      "    ... ).select(df.name, df3.age).show()\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  2|\n",
      "    |  Bob|  5|\n",
      "    +-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6c1385dc-6a93-4749-a310-55482a93d2cf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot resolve column name \"name\" among (id, department, employee_name, rank_number).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munionByName\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/dataframe.py:4037\u001b[0m, in \u001b[0;36mDataFrame.unionByName\u001b[0;34m(self, other, allowMissingColumns)\u001b[0m\n\u001b[1;32m   3959\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munionByName\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m, allowMissingColumns: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3960\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` containing union of rows in this and another\u001b[39;00m\n\u001b[1;32m   3961\u001b[0m \u001b[38;5;124;03m    :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3962\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4035\u001b[0m \u001b[38;5;124;03m    +----+----+----+----+----+----+\u001b[39;00m\n\u001b[1;32m   4036\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munionByName\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowMissingColumns\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot resolve column name \"name\" among (id, department, employee_name, rank_number)."
     ]
    }
   ],
   "source": [
    "df1.unionByName(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d0ae3a72-8ed5-4d79-a994-30706f070686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col0|col1|col2|col3|\n",
      "+----+----+----+----+\n",
      "|   5|   2|   6|NULL|\n",
      "|NULL|   6|   7|   3|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrames with different column names\n",
    "df1 = spark.createDataFrame([[5, 2, 6]], [\"col0\", \"col1\", \"col2\"])\n",
    "df2 = spark.createDataFrame([[6, 7, 3]], [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "# Using allowMissingColumns\n",
    "df3 = df1.unionByName(df2, allowMissingColumns=True)\n",
    "df3.printSchema\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e9c0b41e-35a2-4e14-b90f-72cb885f0784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- department: long (nullable = true)\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- rank_number: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d246816a-1ee2-48e1-a72e-c9c7427e0da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = range(1, 10)\n",
    "rdd = sc.parallelize(li)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c548c200-3f7c-442f-8936-6d13eec2ed1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
