{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eca56c91-075b-4e49-9b60-65b566d8c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName('UDF_Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dd7d64e3-b6e4-431b-8664-9f4f66db57eb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function udf in module pyspark.sql.functions:\n",
      "\n",
      "udf(f: Union[Callable[..., Any], ForwardRef('DataTypeOrString'), NoneType] = None, returnType: 'DataTypeOrString' = StringType(), *, useArrow: Optional[bool] = None) -> Union[ForwardRef('UserDefinedFunctionLike'), Callable[[Callable[..., Any]], ForwardRef('UserDefinedFunctionLike')]]\n",
      "    Creates a user defined function (UDF).\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    f : function\n",
      "        python function if used as a standalone function\n",
      "    returnType : :class:`pyspark.sql.types.DataType` or str\n",
      "        the return type of the user-defined function. The value can be either a\n",
      "        :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "    useArrow : bool or None\n",
      "        whether to use Arrow to optimize the (de)serialization. When it is None, the\n",
      "        Spark config \"spark.sql.execution.pythonUDF.arrow.enabled\" takes effect.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from pyspark.sql.types import IntegerType\n",
      "    >>> slen = udf(lambda s: len(s), IntegerType())\n",
      "    >>> @udf\n",
      "    ... def to_upper(s):\n",
      "    ...     if s is not None:\n",
      "    ...         return s.upper()\n",
      "    ...\n",
      "    >>> @udf(returnType=IntegerType())\n",
      "    ... def add_one(x):\n",
      "    ...     if x is not None:\n",
      "    ...         return x + 1\n",
      "    ...\n",
      "    >>> df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n",
      "    >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")).show()\n",
      "    +----------+--------------+------------+\n",
      "    |slen(name)|to_upper(name)|add_one(age)|\n",
      "    +----------+--------------+------------+\n",
      "    |         8|      JOHN DOE|          22|\n",
      "    +----------+--------------+------------+\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The user-defined functions are considered deterministic by default. Due to\n",
      "    optimization, duplicate invocations may be eliminated or the function may even be invoked\n",
      "    more times than it is present in the query. If your function is not deterministic, call\n",
      "    `asNondeterministic` on the user defined function. E.g.:\n",
      "    \n",
      "    >>> from pyspark.sql.types import IntegerType\n",
      "    >>> import random\n",
      "    >>> random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()\n",
      "    \n",
      "    The user-defined functions do not support conditional expressions or short circuiting\n",
      "    in boolean expressions and it ends up with being executed all internally. If the functions\n",
      "    can fail on special rows, the workaround is to incorporate the condition into the functions.\n",
      "    \n",
      "    The user-defined functions do not take keyword arguments on the calling side.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d5274d94-f89c-447c-bbdc-3b3cdfd78390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5efff3e8-66cd-4144-9d0d-5e0e3673872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertCase(str):\n",
    "    resStr = ''\n",
    "    arr = str.split(' ')\n",
    "    for x in arr:\n",
    "        resStr = resStr + x[0:1].upper()+x[1:len(x)]+' '\n",
    "    return resStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7e2ebda1-68bb-43d0-b7fa-4c9a2ac46655",
   "metadata": {},
   "outputs": [],
   "source": [
    "convertUDF = udf(lambda z:convertCase(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "59cea01b-5c22-49e4-9b37-9af5e0abf815",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|Seqno|         Name|\n",
      "+-----+-------------+\n",
      "|    1|  John Jones |\n",
      "|    2|Tracey Smith |\n",
      "|    3| Amy Sanders |\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('Seqno'), convertUDF(col('Name')).alias('Name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0c451fe7-ca1b-4917-879b-53e68064d4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upperCase(str):\n",
    "    return str.upper()\n",
    "\n",
    "upperCaseUDF = udf(lambda z: upperCase(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "57accdfd-6fcc-48c4-8dd3-aa869de827f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+\n",
      "|Seqno|        Name|curated Name|\n",
      "+-----+------------+------------+\n",
      "|    1|  john jones|  JOHN JONES|\n",
      "|    2|tracey smith|TRACEY SMITH|\n",
      "|    3| amy sanders| AMY SANDERS|\n",
      "+-----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('curated Name', upperCaseUDF(col('Name'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a5dbc768-4a69-4013-9e8a-f2b4d191ea1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/27 19:23:27 WARN SimpleFunctionRegistry: The function convertudfsql replaced a previously registered function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|seqno|         name|\n",
      "+-----+-------------+\n",
      "|    1|  John Jones |\n",
      "|    2|Tracey Smith |\n",
      "|    3| Amy Sanders |\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register('convertUDFSQL', convertCase)\n",
    "df.createOrReplaceTempView('name_table')\n",
    "spark.sql('select seqno, convertUDFSQL(name) as name from name_table').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f7f10ad4-dada-47e4-b2b4-8cc446f09571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|        name|         name|\n",
      "+------------+-------------+\n",
      "|  john jones|  John Jones |\n",
      "|tracey smith|Tracey Smith |\n",
      "| amy sanders| Amy Sanders |\n",
      "+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('name', convertUDF(col('name')).alias('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c5dd78d0-ac43-4626-9a72-a5354c688ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+\n",
      "|Seqno|        Name|Curated name|\n",
      "+-----+------------+------------+\n",
      "|    1|  john jones|  JOHN JONES|\n",
      "|    2|tracey smith|TRACEY SMITH|\n",
      "|    3| amy sanders| AMY SANDERS|\n",
      "+-----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@udf\n",
    "def upperC(str):\n",
    "    return str.upper() if str is not None else ' '\n",
    "\n",
    "df.withColumn('Curated name', upperC(col('name'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8cb16830-93e2-4662-91d8-b93fd2f95c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+\n",
      "|Seqno|        Name|Curated name|\n",
      "+-----+------------+------------+\n",
      "|    1|  john jones|  JOHN JONES|\n",
      "|    2|tracey smith|TRACEY SMITH|\n",
      "|    3| amy sanders| AMY SANDERS|\n",
      "+-----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.withColumn('Curated name', upperC(col('name'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "119e2e41-ee34-4bcf-9e5f-666782478c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|numbers|\n",
      "+-------+\n",
      "|      6|\n",
      "|      8|\n",
      "|     10|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1,), (2,), (3,), (4,), (5,), (6,), (7,), (8,), (9,), (10,)]\n",
    "columns = [\"numbers\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Define a filter condition using boolean expressions\n",
    "filtered_df = df.filter((col(\"numbers\") % 2 == 0) & (col(\"numbers\") > 5))\n",
    "\n",
    "# Show the result\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "151c83f6-9cf1-485d-9963-d825df136258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T\n"
     ]
    }
   ],
   "source": [
    "x = 't'\n",
    "print(x.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a89857f4-3a85-4561-9ec5-239e1eb98baf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "|4    |NULL        |\n",
      "+-----+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/27 19:29:57 ERROR Executor: Exception in task 10.0 in stage 101.0 (TID 543)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4665/1347373182.py\", line 3, in convertCase\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/01/27 19:29:57 WARN TaskSetManager: Lost task 10.0 in stage 101.0 (TID 543) (172.17.35.137 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4665/1347373182.py\", line 3, in convertCase\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/01/27 19:29:57 ERROR TaskSetManager: Task 10 in stage 101.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_4665/1347373182.py\", line 3, in convertCase\nAttributeError: 'NoneType' object has no attribute 'split'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m df2\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNAME_TABLE2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m spark\u001b[38;5;241m.\u001b[39mudf\u001b[38;5;241m.\u001b[39mregister(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconvertUDFS\u001b[39m\u001b[38;5;124m'\u001b[39m,convertCase)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mselect convertUDFS(Name) from NAME_TABLE2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/dataframe.py:972\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    965\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    966\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    969\u001b[0m         },\n\u001b[1;32m    970\u001b[0m     )\n\u001b[0;32m--> 972\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_4665/1347373182.py\", line 3, in convertCase\nAttributeError: 'NoneType' object has no attribute 'split'\n"
     ]
    }
   ],
   "source": [
    "\"\"\" null check \"\"\"\n",
    "\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\"),\n",
    "    ('4',None)]\n",
    "\n",
    "df2 = spark.createDataFrame(data=data,schema=columns)\n",
    "df2.show(truncate=False)\n",
    "df2.createOrReplaceTempView(\"NAME_TABLE2\")\n",
    "spark.udf.register('convertUDFS',convertCase)\n",
    "spark.sql(\"select convertUDFS(Name) from NAME_TABLE2\") \\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1eab12cf-17e9-47eb-a73d-4a3b170cd22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(str)>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"_nullsafeUDF\", lambda str: convertCase(str) if not str is None else \"\" , StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "528bf94f-c0cb-49a7-9149-c3b3ba5793d9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|_nullsafeUDF(Name)|\n",
      "+------------------+\n",
      "|John Jones        |\n",
      "|Tracey Smith      |\n",
      "|Amy Sanders       |\n",
      "|                  |\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select _nullsafeUDF(Name) from NAME_TABLE2\") \\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "241f3064-a79d-4bda-9210-a171b4c2fd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "import random\n",
    "random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
