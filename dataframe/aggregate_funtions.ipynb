{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e79faffa-de22-410e-99c0-74ffe6e4a71c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/11 11:45:35 WARN Utils: Your hostname, Tech-Buddy resolves to a loopback address: 127.0.1.1; using 172.17.35.137 instead (on interface eth0)\n",
      "24/01/11 11:45:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/11 11:45:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.master('local[1]').appName('bbi').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5df4aa35-e619-4362-8d97-9db452ae4c40",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate Functions in PySpark:\n",
      "451\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Get all attributes (functions) in the pyspark.sql.functions module\n",
    "all_functions = [attr for attr in dir(F) if callable(getattr(F, attr))]\n",
    "\n",
    "# Filter only for functions (excluding classes, private methods, etc.)\n",
    "aggregate_functions = [func for func in all_functions if not func.startswith(\"_\") and callable(getattr(F, func))]\n",
    "li = []\n",
    "# Print the names of aggregate functions\n",
    "print(\"Aggregate Functions in PySpark:\")\n",
    "for func_name in aggregate_functions:\n",
    "    li.append(func_name)\n",
    "print(len(li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1121f92-8930-4aab-8c93-a33ce4db5203",
   "metadata": {
    "collapsed": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "| id|category|value|\n",
      "+---+--------+-----+\n",
      "|  1|       A|   10|\n",
      "|  2|       B|   20|\n",
      "|  3|       A|   30|\n",
      "|  4|       B|   40|\n",
      "|  5|       A|   50|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "    (1, \"A\", 10),\n",
    "    (2, \"B\", 20),\n",
    "    (3, \"A\", 30),\n",
    "    (4, \"B\", 40),\n",
    "    (5, \"A\", 50)\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "columns = [\"id\", \"category\", \"value\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48777498-6c2f-45c3-b16a-8509ec95dd6e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Sum:\n",
      "+-----------+\n",
      "|total_value|\n",
      "+-----------+\n",
      "|        150|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Sum\n",
    "result_sum = df.select(sum(\"value\").alias(\"total_value\"))\n",
    "print(\"\\n1. Sum:\")\n",
    "result_sum.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47c0c243-8dc5-4dab-baa1-f89509b0ab3b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Average:\n",
      "+-------------+\n",
      "|average_value|\n",
      "+-------------+\n",
      "|         30.0|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Average\n",
    "result_avg = df.select(avg(\"value\").alias(\"average_value\"))\n",
    "print(\"\\n2. Average:\")\n",
    "result_avg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bdb8698-9244-47a4-bed4-5f72b7f9c9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Maximum:\n",
      "+---------+\n",
      "|max_value|\n",
      "+---------+\n",
      "|       50|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Maximum\n",
    "result_max = df.select(max(\"value\").alias(\"max_value\"))\n",
    "print(\"\\n3. Maximum:\")\n",
    "result_max.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a99ca90-b158-4c1f-8ec1-f968d23a159c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Minimum:\n",
      "+---------+\n",
      "|min_value|\n",
      "+---------+\n",
      "|       10|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Minimum\n",
    "result_min = df.select(min(\"value\").alias(\"min_value\"))\n",
    "print(\"\\n4. Minimum:\")\n",
    "result_min.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "109032e9-220b-466c-be07-b30e431141c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Count:\n",
      "+---------+\n",
      "|row_count|\n",
      "+---------+\n",
      "|        5|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Count\n",
    "result_count = df.select(count(\"*\").alias(\"row_count\"))\n",
    "print(\"\\n5. Count:\")\n",
    "result_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e277561-6346-4a96-9a30-d505b26641b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. CountDistinct:\n",
      "+-------------------+\n",
      "|distinct_categories|\n",
      "+-------------------+\n",
      "|                  2|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. CountDistinct\n",
    "result_count_distinct = df.select(countDistinct(\"category\").alias(\"distinct_categories\"))\n",
    "print(\"\\n6. CountDistinct:\")\n",
    "result_count_distinct.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77398933-65d3-4500-a029-64dd781eb1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "| id|category|value|\n",
      "+---+--------+-----+\n",
      "|  1|       A|   10|\n",
      "|  2|       B|   20|\n",
      "|  3|       A|   30|\n",
      "|  4|       B|   40|\n",
      "|  5|       A|   50|\n",
      "+---+--------+-----+\n",
      "\n",
      "\n",
      "7. First:\n",
      "+-----------+\n",
      "|first_value|\n",
      "+-----------+\n",
      "|         10|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. First\n",
    "result_first = df.select(first(\"value\").alias(\"first_value\"))\n",
    "df.show()\n",
    "print(\"\\n7. First:\")\n",
    "result_first.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cddcc59-0bd7-4413-b24d-df119fb13355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. Last:\n",
      "+----------+\n",
      "|last_value|\n",
      "+----------+\n",
      "|        50|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. Last\n",
    "result_last = df.select(last(\"value\").alias(\"last_value\"))\n",
    "print(\"\\n8. Last:\")\n",
    "result_last.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "671585df-f84f-41a6-a297-1d8408c033f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9. Mean:\n",
      "+----------+\n",
      "|mean_value|\n",
      "+----------+\n",
      "|      30.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9. Mean\n",
    "result_mean = df.select(mean(\"value\").alias(\"mean_value\"))\n",
    "print(\"\\n9. Mean:\")\n",
    "result_mean.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ef34de6-4ef8-4471-8a82-61cfdce09dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10. Standard Deviation:\n",
      "+------------------+\n",
      "|      stddev_value|\n",
      "+------------------+\n",
      "|15.811388300841896|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10. Standard Deviation\n",
    "result_stddev = df.select(stddev(\"value\").alias(\"stddev_value\"))\n",
    "print(\"\\n10. Standard Deviation:\")\n",
    "result_stddev.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd47abf8-6bcb-4081-9701-7ff2c7a802ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11. Variance:\n",
      "+--------------+\n",
      "|variance_value|\n",
      "+--------------+\n",
      "|         250.0|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11. Variance\n",
    "result_variance = df.select(variance(\"value\").alias(\"variance_value\"))\n",
    "print(\"\\n11. Variance:\")\n",
    "result_variance.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25c1166d-3a34-4efc-ba74-3b7a4d21fe28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12. Correlation:\n",
      "+-----------+\n",
      "|correlation|\n",
      "+-----------+\n",
      "|        1.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 12. Correlation\n",
    "result_corr = df.select(corr(\"id\", \"value\").alias(\"correlation\"))\n",
    "print(\"\\n12. Correlation:\")\n",
    "result_corr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2b7f0c0-7b5f-4169-855b-4fb903127e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13. Covariance:\n",
      "+----------+\n",
      "|covariance|\n",
      "+----------+\n",
      "|      20.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 13. Covariance\n",
    "result_covar = df.select(covar_pop(\"id\", \"value\").alias(\"covariance\"))\n",
    "print(\"\\n13. Covariance:\")\n",
    "result_covar.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9937de4c-df16-4bf2-a5a7-65f90591c470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14. Collect List:\n",
      "+--------+------------+\n",
      "|category|values_list |\n",
      "+--------+------------+\n",
      "|B       |[20, 40]    |\n",
      "|A       |[10, 30, 50]|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 14. Collect List\n",
    "result_collect_list = df.groupBy(\"category\").agg(collect_list(\"value\").alias(\"values_list\"))\n",
    "print(\"\\n14. Collect List:\")\n",
    "result_collect_list.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c23f4be4-0276-4bda-8e14-b6c589274405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "15. Collect Set:\n",
      "+--------+------------+\n",
      "|category|values_set  |\n",
      "+--------+------------+\n",
      "|B       |[20, 40]    |\n",
      "|A       |[30, 50, 10]|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 15. Collect Set\n",
    "result_collect_set = df.groupBy(\"category\").agg(collect_set(\"value\").alias(\"values_set\"))\n",
    "print(\"\\n15. Collect Set:\")\n",
    "result_collect_set.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84ec3f11-60e2-4513-970b-c47d132e1d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16. Skewness:\n",
      "+--------------+\n",
      "|skewness_value|\n",
      "+--------------+\n",
      "|           0.0|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 16. Skewness\n",
    "result_skewness = df.select(skewness(\"value\").alias(\"skewness_value\"))\n",
    "print(\"\\n16. Skewness:\")\n",
    "result_skewness.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdd9c74d-5d0d-4c29-85d6-d5be98098af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "17. Kurtosis:\n",
      "+--------------+\n",
      "|kurtosis_value|\n",
      "+--------------+\n",
      "|          -1.3|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 17. Kurtosis\n",
    "result_kurtosis = df.select(kurtosis(\"value\").alias(\"kurtosis_value\"))\n",
    "print(\"\\n17. Kurtosis:\")\n",
    "result_kurtosis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc4e3cb7-5246-4684-a65c-f634e0fa2db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "18. Approximate Quantile:\n",
      "[20.0, 30.0, 40.0]\n"
     ]
    }
   ],
   "source": [
    "# 18. Approximate Quantile\n",
    "result_quantile = df.stat.approxQuantile(\"value\", [0.25, 0.5, 0.75], 0.01)\n",
    "print(\"\\n18. Approximate Quantile:\")\n",
    "print(result_quantile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6355ff1-529a-4930-844f-83b089ab9013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/11 00:58:26 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+----------+\n",
      "| id|category|value|dense_rank|\n",
      "+---+--------+-----+----------+\n",
      "|  1|       A|  100|         1|\n",
      "|  5|       A|  120|         2|\n",
      "|  2|       A|  150|         3|\n",
      "|  6|       B|  160|         1|\n",
      "|  4|       B|  180|         2|\n",
      "|  3|       B|  200|         3|\n",
      "+---+--------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"partitionby_dense_rank_example\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, \"A\", 100),\n",
    "    (2, \"A\", 150),\n",
    "    (3, \"B\", 200),\n",
    "    (4, \"B\", 180),\n",
    "    (5, \"A\", 120),\n",
    "    (6, \"B\", 160),\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "columns = [\"id\", \"category\", \"value\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Define a window specification with partitionBy\n",
    "window_spec = Window().partitionBy(\"category\").orderBy(\"value\")\n",
    "\n",
    "# Use dense_rank function with partitionBy\n",
    "result = df.withColumn(\"dense_rank\", dense_rank().over(window_spec))\n",
    "\n",
    "# Show the result\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58a10d3b-b390-466d-94e8-b13a864e2143",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordered DataFrame (Ascending):\n",
      "+---+--------+-----+\n",
      "| id|category|value|\n",
      "+---+--------+-----+\n",
      "|  1|       A|  100|\n",
      "|  5|       A|  120|\n",
      "|  2|       B|  150|\n",
      "|  6|       B|  160|\n",
      "|  4|       B|  180|\n",
      "|  3|       A|  200|\n",
      "+---+--------+-----+\n",
      "\n",
      "\n",
      "Ordered DataFrame (Descending):\n",
      "+---+--------+-----+\n",
      "| id|category|value|\n",
      "+---+--------+-----+\n",
      "|  3|       A|  200|\n",
      "|  4|       B|  180|\n",
      "|  6|       B|  160|\n",
      "|  2|       B|  150|\n",
      "|  5|       A|  120|\n",
      "|  1|       A|  100|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Order the DataFrame by the \"value\" column in ascending order\n",
    "result_asc = df.orderBy(\"value\")\n",
    "\n",
    "# Show the result\n",
    "print(\"Ordered DataFrame (Ascending):\")\n",
    "result_asc.show()\n",
    "\n",
    "# Order the DataFrame by the \"value\" column in descending order\n",
    "result_desc = df.orderBy(col(\"value\").desc())\n",
    "\n",
    "# Show the result\n",
    "print(\"\\nOrdered DataFrame (Descending):\")\n",
    "result_desc.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "34902493-d57b-46b8-9d7f-a2e741b72f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+---------+\n",
      "| id|category|value|sum_value|\n",
      "+---+--------+-----+---------+\n",
      "|  1|       A|  100|      100|\n",
      "|  3|       A|  200|      200|\n",
      "|  5|       A|  120|      120|\n",
      "|  2|       B|  150|      150|\n",
      "|  4|       B|  180|      180|\n",
      "|  6|       B|  160|      160|\n",
      "+---+--------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a window specification with rangeBetween\n",
    "window_spec = Window().partitionBy(\"category\").orderBy(\"id\").rangeBetween(-1, 1)\n",
    "\n",
    "# Use sum window function with rangeBetween\n",
    "result = df.withColumn(\"sum_value\", sum(\"value\").over(window_spec))\n",
    "\n",
    "# Show the result\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83d2d665-9394-411d-a9e5-79313eadc5a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+---------+\n",
      "| id|category|value|sum_value|\n",
      "+---+--------+-----+---------+\n",
      "|  1|       A|  100|      300|\n",
      "|  3|       A|  200|      420|\n",
      "|  5|       A|  120|      320|\n",
      "|  2|       B|  150|      330|\n",
      "|  4|       B|  180|      490|\n",
      "|  6|       B|  160|      340|\n",
      "+---+--------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a window specification with rowsBetween\n",
    "window_spec = Window().partitionBy(\"category\").orderBy(\"id\").rowsBetween(-1, 1)\n",
    "# Use sum window function with rowsBetween\n",
    "result = df.withColumn(\"sum_value\", sum(\"value\").over(window_spec))\n",
    "\n",
    "# Show the result\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa02739c-58ca-49c6-a713-4b4c8f224fde",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/11 21:51:58 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------------+----+\n",
      "|product_id|   category|sales_amount|rank|\n",
      "+----------+-----------+------------+----+\n",
      "|         6|   Clothing|         400|   1|\n",
      "|         8|   Clothing|         400|   1|\n",
      "|         7|   Clothing|         350|   3|\n",
      "|         4|   Clothing|         300|   4|\n",
      "|         5|   Clothing|         250|   5|\n",
      "|         2|Electronics|         700|   1|\n",
      "|         3|Electronics|         600|   2|\n",
      "|         1|Electronics|         500|   3|\n",
      "+----------+-----------+------------+----+\n",
      "\n",
      "+----------+--------+------------+----+\n",
      "|product_id|category|sales_amount|rank|\n",
      "+----------+--------+------------+----+\n",
      "|         4|Clothing|         300|   4|\n",
      "+----------+--------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"rank_example\").getOrCreate()\n",
    "\n",
    "# Sample sales data with ties in rank 3\n",
    "data = [\n",
    "    (1, \"Electronics\", 500),\n",
    "    (2, \"Electronics\", 700),\n",
    "    (3, \"Electronics\", 600),\n",
    "    (4, \"Clothing\", 300),\n",
    "    (5, \"Clothing\", 250),\n",
    "    (6, \"Clothing\", 400),\n",
    "    (7, \"Clothing\", 350),\n",
    "    (8, \"Clothing\", 400),  # Adding a tied row for rank 3\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "columns = [\"product_id\", \"category\", \"sales_amount\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Define a window specification for ranking within each category\n",
    "window_spec = Window().partitionBy(\"category\").orderBy(col(\"sales_amount\").desc())\n",
    "\n",
    "# Use rank function to assign ranks to products within each category\n",
    "df_ranked = df.withColumn(\"rank\", rank().over(window_spec))\n",
    "df_ranked.show()\n",
    "# Filter for the details of the rank 4 product\n",
    "rank_4_details = df_ranked.filter(col(\"rank\") == 4)\n",
    "\n",
    "# Show the result\n",
    "rank_4_details.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
